{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7da849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47f01809",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacols = ['target','agegroup','fnlwgtgroup','workclass','education','maritalstatus','occupation','relationship','race','sex','cptgaingroup','cptlossgroup','hrsgroup','nativecountry']\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = len(datacols)\n",
    "\n",
    "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "seed = 58\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 128\n",
    "num_epochs = 500\n",
    "projection_dim = 300\n",
    "num_heads = 6\n",
    "\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "\n",
    "transformer_layers = 8\n",
    "\n",
    "embed_dim = projection_dim \n",
    "embeddings_shape = (1,embed_dim)\n",
    "\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n",
    "\n",
    "ckp_path = 'models/Model_Embedding_transformers_v3.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6f275f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5186708",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "        \n",
    "    def __init__(self, df_X, batch_size=32, shuffle=False):\n",
    "        self.batch_size = batch_size\n",
    "        self.df_X = df_X\n",
    "        self.indices = self.df_X.index.tolist()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.indices) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.index[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch = [self.indices[k] for k in index]\n",
    "        \n",
    "        X, y = self.__get_data(batch)\n",
    "        return X, y\n",
    "    \n",
    "    def n(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.index = np.arange(len(self.indices))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.index)\n",
    "\n",
    "    def __get_data(self, batch):\n",
    "        X1 = []\n",
    "        y1 = []\n",
    "        \n",
    "        for i, id in enumerate(batch):\n",
    "            \n",
    "            column = random.randrange(MAX_SEQUENCE_LENGTH)\n",
    "            \n",
    "            # Data\n",
    "            texts = self.df_X.iloc[self.indices[id]]\n",
    "            \n",
    "            #Labels\n",
    "            label = texts[column]\n",
    "            texts[column] = 0\n",
    "\n",
    "            X1.append(texts)\n",
    "            y1.append(label)\n",
    "\n",
    "                            \n",
    "        return np.array(X1), np.array(y1).reshape(self.batch_size,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "790f21fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super().__init__()\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        return positions + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75a06fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingFixedWeights(layers.Layer):\n",
    "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
    "        super(PositionEmbeddingFixedWeights, self).__init__(**kwargs)\n",
    "        \n",
    "        position_embedding_matrix = self.get_position_encoding(sequence_length, output_dim) \n",
    "        \n",
    "        self.position_embedding_layer = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim,\n",
    "            weights=[position_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "             \n",
    "    def get_position_encoding(self, seq_len, d, n=10000):\n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(d/2)):\n",
    "                denominator = np.power(n, 2*i/d)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i+1] = np.cos(k/denominator)\n",
    "        return P\n",
    "\n",
    "\n",
    "    def call(self, inputs):        \n",
    "        position_indices = tf.range(tf.shape(inputs)[1])\n",
    "        embedded_indices = self.position_embedding_layer(position_indices)\n",
    "        return inputs + embedded_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14982440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f03b113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(learning_rate):\n",
    "    \n",
    "    input_text  = layers.Input(shape=(MAX_SEQUENCE_LENGTH-1,),dtype=\"int32\",name='input_text')\n",
    "    \n",
    "    embeddings = tf.keras.layers.Embedding(input_dim=vocab_size, input_length=MAX_SEQUENCE_LENGTH, output_dim=embed_dim,name='embeddings')(input_text)\n",
    "    \n",
    "    embeddings = PositionEmbeddingFixedWeights(sequence_length=MAX_SEQUENCE_LENGTH-1,output_dim=embed_dim)(embeddings)\n",
    "    \n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(embeddings)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1)(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, embeddings])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        embeddings = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(embeddings)\n",
    "    representation = layers.GlobalAveragePooling1D()(representation)\n",
    "    representation = layers.Dropout(0.1)(representation)\n",
    "    \n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.1)\n",
    "    \n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(num_classes,activation='softmax', name='activation')(features)\n",
    "    \n",
    "    # Create the Keras model.\n",
    "    model = Model(inputs=input_text, outputs=outputs,name='Customer_Code_Embeddings')\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate),loss=\"sparse_categorical_crossentropy\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16648b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train_data_cat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "428af28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the lines of text\n",
    "data['data'] = data[datacols].apply(' '.join, axis=1)\n",
    "texts = pd.DataFrame()\n",
    "texts['data'] = data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3a8f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lines = []\n",
    "for index, row in texts.iterrows():\n",
    "    text_lines.append(row['data'].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ece082da",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = [item for text_lines in text_lines for item in text_lines]\n",
    "texts = list(np.unique(np.array(text_tokens)))\n",
    "VOCAB_SIZE = len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c2e1c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n"
     ]
    }
   ],
   "source": [
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4faf6ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tokenizer\n",
    "tokenizer = Tokenizer(filters=' ')\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "675643e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_docs = tokenizer.texts_to_sequences(text_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac6d22d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode the documents\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "num_classes = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b06ca524",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_docs = pad_sequences(encoded_docs, maxlen=MAX_SEQUENCE_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62fb38b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data generator\n",
    "train = pd.DataFrame(padded_docs)\n",
    "train_generator = DataGenerator(df_X=train, batch_size=batch_size, shuffle=True)\n",
    "STEP_SIZE_TRAIN=train_generator.n()//train_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16c61213",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "381/381 [==============================] - 51s 107ms/step - loss: 3.6475 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "381/381 [==============================] - 41s 107ms/step - loss: 3.6576 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "381/381 [==============================] - 41s 108ms/step - loss: 3.5685 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "381/381 [==============================] - 42s 109ms/step - loss: 3.4532 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "381/381 [==============================] - 41s 107ms/step - loss: 3.3059 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "381/381 [==============================] - 42s 111ms/step - loss: 3.1838 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "381/381 [==============================] - 42s 110ms/step - loss: 3.0365 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "381/381 [==============================] - 42s 110ms/step - loss: 2.9012 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "381/381 [==============================] - 42s 111ms/step - loss: 2.7526 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "381/381 [==============================] - 42s 111ms/step - loss: 2.6040 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "381/381 [==============================] - 43s 112ms/step - loss: 2.4741 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "381/381 [==============================] - 42s 111ms/step - loss: 2.3538 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "381/381 [==============================] - 42s 111ms/step - loss: 2.2293 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "381/381 [==============================] - 43s 112ms/step - loss: 2.0858 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "381/381 [==============================] - 43s 112ms/step - loss: 1.9898 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "381/381 [==============================] - 43s 113ms/step - loss: 1.8673 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "381/381 [==============================] - 42s 111ms/step - loss: 1.7643 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "381/381 [==============================] - 42s 110ms/step - loss: 1.6546 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "381/381 [==============================] - 42s 111ms/step - loss: 1.5725 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "381/381 [==============================] - 42s 109ms/step - loss: 1.5861 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "381/381 [==============================] - 42s 111ms/step - loss: 1.3994 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "381/381 [==============================] - 42s 111ms/step - loss: 1.3077 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "381/381 [==============================] - 42s 110ms/step - loss: 1.2441 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "381/381 [==============================] - 42s 111ms/step - loss: 1.1792 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "381/381 [==============================] - 42s 111ms/step - loss: 1.0953 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "381/381 [==============================] - 41s 108ms/step - loss: 1.0263 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "381/381 [==============================] - 42s 111ms/step - loss: 0.9567 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "381/381 [==============================] - 41s 109ms/step - loss: 0.9080 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "381/381 [==============================] - 41s 108ms/step - loss: 0.8495 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "381/381 [==============================] - 41s 109ms/step - loss: 0.8084 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "381/381 [==============================] - 41s 108ms/step - loss: 0.7410 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "381/381 [==============================] - 42s 109ms/step - loss: 0.7109 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "381/381 [==============================] - 42s 109ms/step - loss: 0.6783 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "381/381 [==============================] - 41s 108ms/step - loss: 0.6237 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "381/381 [==============================] - 42s 110ms/step - loss: 0.6011 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "381/381 [==============================] - 42s 110ms/step - loss: 0.5599 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "381/381 [==============================] - 41s 109ms/step - loss: 0.5104 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "381/381 [==============================] - 41s 108ms/step - loss: 0.4891 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "381/381 [==============================] - 41s 108ms/step - loss: 0.4623 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "381/381 [==============================] - 41s 108ms/step - loss: 0.4321 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "381/381 [==============================] - 41s 108ms/step - loss: 0.3933 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "381/381 [==============================] - 41s 108ms/step - loss: 0.3716 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "381/381 [==============================] - 41s 108ms/step - loss: 0.3522 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "381/381 [==============================] - 41s 108ms/step - loss: 0.3401 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "381/381 [==============================] - 41s 108ms/step - loss: 0.3214 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "381/381 [==============================] - 41s 107ms/step - loss: 0.2870 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "381/381 [==============================] - 41s 108ms/step - loss: 0.2698 - lr: 0.0010\n",
      "Epoch 48/500\n",
      "381/381 [==============================] - 41s 108ms/step - loss: 0.2534 - lr: 0.0010\n",
      "Epoch 49/500\n",
      "381/381 [==============================] - 41s 108ms/step - loss: 0.2397 - lr: 0.0010\n",
      "Epoch 50/500\n",
      "381/381 [==============================] - 41s 108ms/step - loss: 0.2217 - lr: 0.0010\n",
      "Epoch 51/500\n",
      "381/381 [==============================] - 41s 107ms/step - loss: 0.2142 - lr: 0.0010\n",
      "Epoch 52/500\n",
      "381/381 [==============================] - 41s 108ms/step - loss: 0.1971 - lr: 0.0010\n",
      "Epoch 53/500\n",
      "381/381 [==============================] - 41s 108ms/step - loss: 0.1870 - lr: 0.0010\n",
      "Epoch 54/500\n",
      "381/381 [==============================] - 41s 109ms/step - loss: 0.1729 - lr: 0.0010\n",
      "Epoch 55/500\n",
      "381/381 [==============================] - 41s 107ms/step - loss: 0.1615 - lr: 0.0010\n",
      "Epoch 56/500\n",
      "381/381 [==============================] - 41s 107ms/step - loss: 0.1462 - lr: 0.0010\n",
      "Epoch 57/500\n",
      "381/381 [==============================] - 42s 109ms/step - loss: 0.1354 - lr: 0.0010\n",
      "Epoch 58/500\n",
      "381/381 [==============================] - 41s 109ms/step - loss: 0.1328 - lr: 0.0010\n",
      "Epoch 59/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.1264 - lr: 0.0010\n",
      "Epoch 60/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.1142 - lr: 0.0010\n",
      "Epoch 61/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.1082 - lr: 0.0010\n",
      "Epoch 62/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.1014 - lr: 0.0010\n",
      "Epoch 63/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0987 - lr: 0.0010\n",
      "Epoch 64/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0864 - lr: 0.0010\n",
      "Epoch 65/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 0.0904 - lr: 0.0010\n",
      "Epoch 66/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0840 - lr: 0.0010\n",
      "Epoch 67/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0683 - lr: 0.0010\n",
      "Epoch 68/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0676 - lr: 0.0010\n",
      "Epoch 69/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0635 - lr: 0.0010\n",
      "Epoch 70/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 0.0661 - lr: 0.0010\n",
      "Epoch 71/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0556 - lr: 0.0010\n",
      "Epoch 72/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0503 - lr: 0.0010\n",
      "Epoch 73/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 0.0505 - lr: 0.0010\n",
      "Epoch 74/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0457 - lr: 0.0010\n",
      "Epoch 75/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 0.0406 - lr: 0.0010\n",
      "Epoch 76/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0382 - lr: 0.0010\n",
      "Epoch 77/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0365 - lr: 0.0010\n",
      "Epoch 78/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0336 - lr: 0.0010\n",
      "Epoch 79/500\n",
      "381/381 [==============================] - 41s 107ms/step - loss: 0.0325 - lr: 0.0010\n",
      "Epoch 80/500\n",
      "381/381 [==============================] - 41s 107ms/step - loss: 0.0297 - lr: 0.0010\n",
      "Epoch 81/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0318 - lr: 0.0010\n",
      "Epoch 82/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0246 - lr: 0.0010\n",
      "Epoch 83/500\n",
      "381/381 [==============================] - 41s 107ms/step - loss: 0.0228 - lr: 0.0010\n",
      "Epoch 84/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0250 - lr: 0.0010\n",
      "Epoch 85/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0245 - lr: 0.0010\n",
      "Epoch 86/500\n",
      "381/381 [==============================] - 41s 107ms/step - loss: 0.0220 - lr: 0.0010\n",
      "Epoch 87/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 0.0255 - lr: 4.0000e-04\n",
      "Epoch 88/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0189 - lr: 4.0000e-04\n",
      "Epoch 89/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0159 - lr: 4.0000e-04\n",
      "Epoch 90/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 0.0159 - lr: 4.0000e-04\n",
      "Epoch 91/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 0.0154 - lr: 4.0000e-04\n",
      "Epoch 92/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0100 - lr: 4.0000e-04\n",
      "Epoch 93/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 0.0167 - lr: 4.0000e-04\n",
      "Epoch 94/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 0.0127 - lr: 4.0000e-04\n",
      "Epoch 95/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 0.0105 - lr: 4.0000e-04\n",
      "Epoch 96/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 0.0104 - lr: 1.6000e-04\n",
      "Epoch 97/500\n",
      "381/381 [==============================] - 41s 106ms/step - loss: 0.0080 - lr: 1.6000e-04\n",
      "Epoch 98/500\n",
      "381/381 [==============================] - 41s 106ms/step - loss: 0.0095 - lr: 1.6000e-04\n",
      "Epoch 99/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0101 - lr: 1.6000e-04\n",
      "Epoch 100/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0072 - lr: 1.6000e-04\n",
      "Epoch 101/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 0.0082 - lr: 6.4000e-05\n",
      "Epoch 102/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0072 - lr: 6.4000e-05\n",
      "Epoch 103/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0055 - lr: 6.4000e-05\n",
      "Epoch 104/500\n",
      "381/381 [==============================] - 41s 106ms/step - loss: 0.0052 - lr: 6.4000e-05\n",
      "Epoch 105/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 0.0053 - lr: 6.4000e-05\n",
      "Epoch 106/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 0.0054 - lr: 6.4000e-05\n",
      "Epoch 107/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0051 - lr: 2.5600e-05\n",
      "Epoch 108/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 0.0050 - lr: 2.5600e-05\n",
      "Epoch 109/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0035 - lr: 2.5600e-05\n",
      "Epoch 110/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 0.0057 - lr: 2.5600e-05\n",
      "Epoch 111/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 0.0037 - lr: 2.5600e-05\n",
      "Epoch 112/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0032 - lr: 2.5600e-05\n",
      "Epoch 113/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0039 - lr: 1.0240e-05\n",
      "Epoch 114/500\n",
      "381/381 [==============================] - 41s 106ms/step - loss: 0.0031 - lr: 1.0240e-05\n",
      "Epoch 115/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0034 - lr: 1.0240e-05\n",
      "Epoch 116/500\n",
      "381/381 [==============================] - 41s 106ms/step - loss: 0.0027 - lr: 4.0960e-06\n",
      "Epoch 117/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 0.0033 - lr: 4.0960e-06\n",
      "Epoch 118/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0032 - lr: 4.0960e-06\n",
      "Epoch 119/500\n",
      "381/381 [==============================] - 41s 107ms/step - loss: 0.0015 - lr: 1.6384e-06\n",
      "Epoch 120/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0033 - lr: 1.6384e-06\n",
      "Epoch 121/500\n",
      "381/381 [==============================] - 41s 106ms/step - loss: 0.0020 - lr: 1.6384e-06\n",
      "Epoch 122/500\n",
      "381/381 [==============================] - 41s 107ms/step - loss: 0.0014 - lr: 1.6384e-06\n",
      "Epoch 123/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 0.0034 - lr: 6.5536e-07\n",
      "Epoch 124/500\n",
      "381/381 [==============================] - 41s 106ms/step - loss: 0.0012 - lr: 6.5536e-07\n",
      "Epoch 125/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 0.0017 - lr: 6.5536e-07\n",
      "Epoch 126/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0017 - lr: 2.6214e-07\n",
      "Epoch 127/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0013 - lr: 2.6214e-07\n",
      "Epoch 128/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 0.0021 - lr: 2.6214e-07\n",
      "Epoch 129/500\n",
      "381/381 [==============================] - 41s 106ms/step - loss: 8.2441e-04 - lr: 1.0486e-07\n",
      "Epoch 130/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 0.0016 - lr: 1.0486e-07\n",
      "Epoch 131/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 8.2332e-04 - lr: 1.0486e-07\n",
      "Epoch 132/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 0.0011 - lr: 4.1943e-08\n",
      "Epoch 133/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 6.0601e-04 - lr: 4.1943e-08\n",
      "Epoch 134/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 7.6868e-04 - lr: 4.1943e-08\n",
      "Epoch 135/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 3.0755e-04 - lr: 1.6777e-08\n",
      "Epoch 136/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 5.2303e-04 - lr: 1.6777e-08\n",
      "Epoch 137/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 5.3934e-04 - lr: 1.6777e-08\n",
      "Epoch 138/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 8.0379e-04 - lr: 1.6777e-08\n",
      "Epoch 139/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 0.0010 - lr: 6.7109e-09\n",
      "Epoch 140/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 7.5744e-04 - lr: 6.7109e-09\n",
      "Epoch 141/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 0.0015 - lr: 6.7109e-09\n",
      "Epoch 142/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 5.7874e-04 - lr: 2.6844e-09\n",
      "Epoch 143/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 7.7673e-04 - lr: 2.6844e-09\n",
      "Epoch 144/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 5.1745e-04 - lr: 2.6844e-09\n",
      "Epoch 145/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 5.4996e-04 - lr: 1.0737e-09\n",
      "Epoch 146/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 3.0662e-04 - lr: 1.0737e-09\n",
      "Epoch 147/500\n",
      "381/381 [==============================] - 40s 104ms/step - loss: 3.0672e-04 - lr: 1.0737e-09\n",
      "Epoch 148/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 5.2829e-04 - lr: 4.2950e-10\n",
      "Epoch 149/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 5.1161e-04 - lr: 4.2950e-10\n",
      "Epoch 150/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 0.0010 - lr: 4.2950e-10\n",
      "Epoch 151/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 5.2767e-04 - lr: 1.7180e-10\n",
      "Epoch 152/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 5.1455e-04 - lr: 1.7180e-10\n",
      "Epoch 153/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 3.0590e-04 - lr: 1.7180e-10\n",
      "Epoch 154/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 3.0693e-04 - lr: 6.8719e-11\n",
      "Epoch 155/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 5.6275e-04 - lr: 6.8719e-11\n",
      "Epoch 156/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 5.2477e-04 - lr: 6.8719e-11\n",
      "Epoch 157/500\n",
      "381/381 [==============================] - 40s 104ms/step - loss: 8.0475e-04 - lr: 2.7488e-11\n",
      "Epoch 158/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 3.0709e-04 - lr: 2.7488e-11\n",
      "Epoch 159/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 3.0604e-04 - lr: 2.7488e-11\n",
      "Epoch 160/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 3.0719e-04 - lr: 1.0995e-11\n",
      "Epoch 161/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 3.0625e-04 - lr: 1.0995e-11\n",
      "Epoch 162/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 3.0569e-04 - lr: 1.0995e-11\n",
      "Epoch 163/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 3.0791e-04 - lr: 4.3980e-12\n",
      "Epoch 164/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 3.0780e-04 - lr: 4.3980e-12\n",
      "Epoch 165/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 3.0726e-04 - lr: 4.3980e-12\n",
      "Epoch 166/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 3.0705e-04 - lr: 1.7592e-12\n",
      "Epoch 167/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 5.5332e-04 - lr: 1.7592e-12\n",
      "Epoch 168/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 3.0636e-04 - lr: 1.7592e-12\n",
      "Epoch 169/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 3.0740e-04 - lr: 7.0369e-13\n",
      "Epoch 170/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 3.0709e-04 - lr: 7.0369e-13\n",
      "Epoch 171/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 3.0638e-04 - lr: 7.0369e-13\n",
      "Epoch 172/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 3.0681e-04 - lr: 2.8148e-13\n",
      "Epoch 173/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 3.0640e-04 - lr: 2.8148e-13\n",
      "Epoch 174/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 3.0600e-04 - lr: 2.8148e-13\n",
      "Epoch 175/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 5.3140e-04 - lr: 1.1259e-13\n",
      "Epoch 176/500\n",
      "381/381 [==============================] - 40s 105ms/step - loss: 3.0739e-04 - lr: 1.1259e-13\n",
      "Epoch 177/500\n",
      "381/381 [==============================] - 40s 106ms/step - loss: 5.3528e-04 - lr: 1.1259e-13\n"
     ]
    }
   ],
   "source": [
    "model = create_classifier(learning_rate)\n",
    "        \n",
    "lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'loss', factor = 0.4, patience = 3, verbose = 0, min_delta = 0.001, mode = 'min')\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='min', verbose=0, patience=15, restore_best_weights=True)\n",
    "mc = tf.keras.callbacks.ModelCheckpoint(ckp_path, monitor='loss', mode='min', verbose=0, save_best_only=True, save_weights_only=True)\n",
    "        \n",
    "# train the model\n",
    "history = model.fit(x=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    callbacks=[mc,lr,es],  \n",
    "                    shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9d20534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the model\n",
    "model = create_classifier(learning_rate)\n",
    "model.load_weights(ckp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d732674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embedding layer weights\n",
    "embedding_layer = model.get_layer('embeddings')\n",
    "embeddings = embedding_layer.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ceb69ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the Embeddings\n",
    "np.savetxt(\"data/embeddings_model_w2v_v3.csv\", embeddings, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "311cb1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dictionaries for further processing\n",
    "word_to_index = tokenizer.word_index\n",
    "index_to_word = dict()\n",
    "\n",
    "for key in word_to_index:\n",
    "    index_to_word.update({word_to_index[key] : key })\n",
    "\n",
    "word_to_index.update({'unk':0})\n",
    "index_to_word.update({0:'unk'})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28d31a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dictionaries\n",
    "with open('data/word_to_index_v3.pkl', 'wb') as fp:\n",
    "    pickle.dump(word_to_index, fp)\n",
    "    \n",
    "with open('data/index_to_word_v3.pkl', 'wb') as fp:\n",
    "    pickle.dump(index_to_word, fp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05027c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
