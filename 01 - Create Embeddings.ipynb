{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f01809",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacols = ['agegroup','fnlwgtgroup','workclass','education','maritalstatus','occupation','relationship','race','sex','cptgaingroup','cptlossgroup','nativecountry']\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = len(datacols)\n",
    "\n",
    "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "seed = 58\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 128\n",
    "num_epochs = 500\n",
    "projection_dim = 300\n",
    "num_heads = 6\n",
    "\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "\n",
    "transformer_layers = 8\n",
    "\n",
    "embed_dim = projection_dim \n",
    "embeddings_shape = (1,embed_dim)\n",
    "\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n",
    "\n",
    "ckp_path = 'models/Model_Embedding_transformers_v3.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f275f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5186708",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "        \n",
    "    def __init__(self, df_X, batch_size=32, shuffle=False):\n",
    "        self.batch_size = batch_size\n",
    "        self.df_X = df_X\n",
    "        self.indices = self.df_X.index.tolist()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.indices) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.index[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch = [self.indices[k] for k in index]\n",
    "        \n",
    "        X, y = self.__get_data(batch)\n",
    "        return X, y\n",
    "    \n",
    "    def n(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.index = np.arange(len(self.indices))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.index)\n",
    "\n",
    "    def __get_data(self, batch):\n",
    "        X1 = []\n",
    "        y1 = []\n",
    "        \n",
    "        for i, id in enumerate(batch):\n",
    "            \n",
    "            column = random.randrange(MAX_SEQUENCE_LENGTH)\n",
    "            \n",
    "            # Data\n",
    "            texts = self.df_X.iloc[self.indices[id]]\n",
    "            \n",
    "            #Labels\n",
    "            label = texts[column]\n",
    "            texts[column] = 0\n",
    "\n",
    "            X1.append(texts)\n",
    "            y1.append(label)\n",
    "\n",
    "                            \n",
    "        return np.array(X1), np.array(y1).reshape(self.batch_size,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790f21fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super().__init__()\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        return positions + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a06fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingFixedWeights(layers.Layer):\n",
    "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
    "        super(PositionEmbeddingFixedWeights, self).__init__(**kwargs)\n",
    "        \n",
    "        position_embedding_matrix = self.get_position_encoding(sequence_length, output_dim) \n",
    "        \n",
    "        self.position_embedding_layer = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim,\n",
    "            weights=[position_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "             \n",
    "    def get_position_encoding(self, seq_len, d, n=10000):\n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(d/2)):\n",
    "                denominator = np.power(n, 2*i/d)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i+1] = np.cos(k/denominator)\n",
    "        return P\n",
    "\n",
    "\n",
    "    def call(self, inputs):        \n",
    "        position_indices = tf.range(tf.shape(inputs)[1])\n",
    "        embedded_indices = self.position_embedding_layer(position_indices)\n",
    "        return inputs + embedded_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14982440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03b113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(learning_rate):\n",
    "    \n",
    "    input_text  = layers.Input(shape=(MAX_SEQUENCE_LENGTH-1,),dtype=\"int32\",name='input_text')\n",
    "    \n",
    "    embeddings = tf.keras.layers.Embedding(input_dim=vocab_size, input_length=MAX_SEQUENCE_LENGTH, output_dim=embed_dim,name='embeddings')(input_text)\n",
    "    \n",
    "    embeddings = PositionEmbeddingFixedWeights(sequence_length=MAX_SEQUENCE_LENGTH-1,output_dim=embed_dim)(embeddings)\n",
    "    \n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(embeddings)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1)(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, embeddings])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        embeddings = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(embeddings)\n",
    "    representation = layers.GlobalAveragePooling1D()(representation)\n",
    "    representation = layers.Dropout(0.1)(representation)\n",
    "    \n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.1)\n",
    "    \n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(num_classes,activation='softmax', name='activation')(features)\n",
    "    \n",
    "    # Create the Keras model.\n",
    "    model = Model(inputs=input_text, outputs=outputs,name='Customer_Code_Embeddings')\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate),loss=\"sparse_categorical_crossentropy\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16648b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train_data_cat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428af28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['data'] = data[datacols].apply(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85552dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pd.DataFrame()\n",
    "texts['data'] = data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a8f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lines = []\n",
    "for index, row in texts.iterrows():\n",
    "    text_lines.append(row['data'].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece082da",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = [item for text_lines in text_lines for item in text_lines]\n",
    "texts = list(np.unique(np.array(text_tokens)))\n",
    "VOCAB_SIZE = len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2e1c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faf6ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tokenizer\n",
    "tokenizer = Tokenizer(filters=' ')\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675643e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_docs = tokenizer.texts_to_sequences(text_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6d22d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode the documents\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "num_classes = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06ca524",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_docs = pad_sequences(encoded_docs, maxlen=MAX_SEQUENCE_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fb38b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(padded_docs)\n",
    "train_generator = DataGenerator(df_X=train, batch_size=batch_size, shuffle=True)\n",
    "STEP_SIZE_TRAIN=train_generator.n()//train_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c61213",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_classifier(learning_rate)\n",
    "        \n",
    "lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'loss', factor = 0.4, patience = 3, verbose = 0, min_delta = 0.001, mode = 'min')\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='min', verbose=0, patience=15, restore_best_weights=True)\n",
    "mc = tf.keras.callbacks.ModelCheckpoint(ckp_path, monitor='loss', mode='min', verbose=0, save_best_only=True, save_weights_only=True)\n",
    "        \n",
    "# train the model\n",
    "history = model.fit(x=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    callbacks=[mc,lr,es],  \n",
    "                    shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d20534",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_classifier(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad662b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(ckp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035967dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = model.get_layer('embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d732674",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embedding_layer.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb69ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"data/embeddings_model_w2v_v3.csv\", embeddings, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311cb1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = tokenizer.word_index\n",
    "index_to_word = dict()\n",
    "\n",
    "for key in word_to_index:\n",
    "    index_to_word.update({word_to_index[key] : key })\n",
    "\n",
    "word_to_index.update({'unk':0})\n",
    "index_to_word.update({0:'unk'})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d31a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/word_to_index_v3.pkl', 'wb') as fp:\n",
    "    pickle.dump(word_to_index, fp)\n",
    "    \n",
    "with open('data/index_to_word_v3.pkl', 'wb') as fp:\n",
    "    pickle.dump(index_to_word, fp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05027c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
